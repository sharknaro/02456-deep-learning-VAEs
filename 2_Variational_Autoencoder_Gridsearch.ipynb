{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1413e46c-6a93-46c3-ac3e-7314c516a08a",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "This was done using Ray Tune with the ASHA Scheduler using a simple grid search approach. <br>\n",
    "We used a random 20% subset of our full data for the hyperparameter optimiaztion due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1597afd6-68ce-4391-8350-2c984887551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, clear_output\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import time\n",
    "import sklearn\n",
    "import ray\n",
    "\n",
    "from typing import *\n",
    "from collections import defaultdict\n",
    "from warnings import simplefilter\n",
    "\n",
    "from ray import tune, air\n",
    "from ray.air import session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.log_normal import LogNormal\n",
    "from torch.utils.data import random_split, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00acee8b-a8dd-4e1c-b193-d3ff4fee4123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 14:06:16,783\tINFO worker.py:1519 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.1.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='2.1.0', ray_commit='be49bde7ee4f6adb3f8710aee0665c27f9f0bb62', address_info={'node_ip_address': '172.27.30.56', 'raylet_ip_address': '172.27.30.56', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-12-21_14-06-14_759755_1111/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-12-21_14-06-14_759755_1111/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-12-21_14-06-14_759755_1111', 'metrics_export_port': 53565, 'gcs_address': '172.27.30.56:55475', 'address': '172.27.30.56:55475', 'dashboard_agent_listen_port': 52365, 'node_id': '402a1a8ed0a6a1bb58bc0898c6d6ddd3299e47fd87f006172fe37c02'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initalize ray tune to run on GPU\n",
    "ray.init(num_gpus = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679e3d3-9f54-4975-8bc1-92c231f853c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function for plotting during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d8d146-10b0-4dbb-9ad7-fedfd1333ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(training_data, validation_data, x, x_hat, tmp_img=\"tmp_vae_out.png\"):\n",
    "    \n",
    "    fig, axes = plt.subplot_mosaic([['top_left', 'top_centre', 'top_right'],\n",
    "                                    ['mid', 'mid', 'mid'],\n",
    "                                    ['bottom', 'bottom', 'bottom']])\n",
    "    \n",
    "    fig.set_size_inches(15, 10)\n",
    "    \n",
    "    # plot ELBO\n",
    "    axes['top_left'].set_title(r'ELBO: $\\mathcal{L} ( \\mathbf{x} )$')\n",
    "    axes['top_left'].plot(training_data['elbo'], label='Training')\n",
    "    axes['top_left'].plot(validation_data['elbo'], label='Validation')\n",
    "    axes['top_left'].legend()\n",
    "    \n",
    "    # plot KL\n",
    "    axes['top_centre'].set_title(r'$\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)$')\n",
    "    axes['top_centre'].plot(training_data['kl'], label='Training')\n",
    "    axes['top_centre'].plot(validation_data['kl'], label='Validation')\n",
    "    axes['top_centre'].legend()\n",
    "    \n",
    "    \n",
    "    # plot NLL\n",
    "    axes['top_right'].set_title(r'$\\log p_\\theta(\\mathbf{x} | \\mathbf{z})$')\n",
    "    axes['top_right'].plot(training_data['log_px'], label='Training')\n",
    "    axes['top_right'].plot(validation_data['log_px'], label='Validation')\n",
    "    axes['top_right'].legend()\n",
    "    \n",
    "    # plot sample\n",
    "    axes['mid'].set_title(r'Observation $\\mathbf{x}$')\n",
    "    axes['mid'].imshow(x, cmap=\"plasma\", aspect=\"auto\")\n",
    "    a = axes['mid'].imshow(x, cmap=\"plasma\", aspect=\"auto\")\n",
    "    plt.colorbar(a, ax = axes['mid'], location='right')\n",
    "    \n",
    "    # plot reconstruction\n",
    "    axes['bottom'].set_title(r'Reconstruction $\\mathbf{\\hat x \\sim p(x|z)}$')\n",
    "    axes['bottom'].imshow(x_hat, cmap=\"plasma\", aspect=\"auto\")\n",
    "    b = axes['bottom'].imshow(x_hat, cmap=\"plasma\", aspect=\"auto\")\n",
    "    plt.colorbar(b, ax = axes['bottom'], location='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(tmp_img)\n",
    "    plt.close(fig)\n",
    "    display(Image(filename=tmp_img))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    os.remove(tmp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd508c-dd50-46be-b7dc-3398d44df886",
   "metadata": {},
   "source": [
    "# Dataloader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c654b762-e32a-4d18-aee6-59ce3d2e29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders(DATA_PATH=os.getcwd() + \"/Data/archs4_data_transformed.npy\", batch_size=32, num_workers=4, pin_memory=True):\n",
    "    # print(\"LOADING DATA ...\")\n",
    "    start = time.time()\n",
    "    data = np.load(DATA_PATH)\n",
    "    end = time.time()\n",
    "\n",
    "    assert np.isnan(np.sum(data)) == False\n",
    "    \n",
    "    inputs = torch.from_numpy(data)\n",
    "    inputs_train, inputs_test = random_split(inputs, [0.8,0.2])\n",
    "    \n",
    "    # Training data:\n",
    "    train_loader = DataLoader(dataset = inputs_train,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory)\n",
    "\n",
    "    # Testing data:\n",
    "    test_loader = DataLoader(dataset = inputs_test,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd7d94-1be2-4196-9f74-afb9ff155842",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining custom distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524bfe06-bc3c-4ca2-916f-c869295b47fc",
   "metadata": {},
   "source": [
    "#### After log2(x+1) and quantile transformation, our data follows a hurdle normal distribution which is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ce8e09-f3ba-47d1-8d37-57f301ce73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HurdleNormal(Distribution):\n",
    "    \"\"\"\n",
    "    A hurdle model of the Bernoulli and normal distributions to model data which is normally distributed with an excess probability mass at 0\n",
    "    Hurdle Normal PDF:\n",
    "    p(x | mu, sigma, p) = p for x=0\n",
    "    p(x | mu, sigma, p) = (1-p) * normal distribution pdf\n",
    "    This is to be used as observation model p(x|z): p(x | z, mu, sigma, p)\n",
    "    \"\"\"\n",
    "    def __init__(self, mu:Tensor, log_sigma:Tensor, p_logits:Tensor):\n",
    "        assert mu.shape == log_sigma.shape == p_logits.shape, f\"Tensors 'mu': {mu.shape}, 'log_sigma': {log_sigma.shape} and 'p': {p_logits.shape} must be of the same shape.\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp() + 0.01 # to avoid sigma being zero\n",
    "        self.p_logits = p_logits\n",
    "\n",
    "        self.Bernoulli = torch.distributions.bernoulli.Bernoulli(logits = self.p_logits)\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `x ~ hurdle_normal(x | mu, sigma, p)`\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Bernoulli returns either 0 or 1, and for 1 we want the value from the sampled log_normal distribution so we just multiply them\n",
    "            B = self.Bernoulli.sample() \n",
    "            norm = torch.distributions.normal.Normal(self.mu, self.sigma).sample()\n",
    "            return B*norm\n",
    "            \n",
    "    def log_prob(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        return log probability: log p(x)\n",
    "        for x=0:\n",
    "            p(x|mu, sigma, p) = p \n",
    "        for x > 0\n",
    "            p(x|mu, sigma, p) = (1-p) * 1 / (sigma * sqrt(2*pi)) * exp(-0.5 * ((x - mu) / sigma)) \n",
    "        \"\"\"\n",
    "        \n",
    "        idx = x > 0.\n",
    "        \n",
    "        # Log prob of zero values = log p\n",
    "        # Using torch.zeros as for the places where x is not 0 we want (1-p)\n",
    "        bernoulli_tensor = torch.zeros_like(x)\n",
    "        bernoulli_tensor[idx] = 1.\n",
    "        \n",
    "        log_prob_bernoulli = self.Bernoulli.log_prob(bernoulli_tensor)\n",
    "        \n",
    "        log_prob_normal = torch.distributions.normal.Normal(loc = self.mu[idx], scale = self.sigma[idx]).log_prob(x[idx])\n",
    "        log_prob_normal = torch.zeros_like(log_prob_bernoulli).masked_scatter(idx, log_prob_normal)\n",
    "        \n",
    "        # FINAL LOG PROB\n",
    "        log_p = log_prob_bernoulli + log_prob_normal # Log prob will be log(p) when x=0 and log(p-1) + logprob_normal when x>0\n",
    "\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c32b975-fcee-4eae-8e27-ba2e81476dd3",
   "metadata": {},
   "source": [
    "#### The reparameterization trick to be able to backpropagate despite random sampling is used as implemented in the course exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f32fd5d-d78d-4c06-8519-a6d56b757410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp() + 0.001\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        return self.mu + self.sigma * self.sample_epsilon() \n",
    "        \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return torch.distributions.normal.Normal(self.mu, self.sigma).log_prob(z)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ReparameterizedDiagonalGaussian(mu={self.mu.shape}, sigma={self.sigma.shape})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccff15e-93e5-45bc-bf65-5dd967a2efce",
   "metadata": {},
   "source": [
    "## Defining the Variational Autoencoder network\n",
    "##### Based on the original code from the course exercise, but modified to be able to control number of layers, dimensions of layers and use the hurdle-normal distribution as observation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106cb8f0-b9b6-4af1-83fd-d711b476fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int, encoder_layer_sizes:list, decoder_layer_sizes:list) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Defining parameters of model\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        \n",
    "        self.encoder_layer_sizes = encoder_layer_sizes\n",
    "        self.n_encoder_layers = len(encoder_layer_sizes)\n",
    "        \n",
    "        self.decoder_layer_sizes = decoder_layer_sizes\n",
    "        self.n_decoder_layers = len(decoder_layer_sizes)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Defining flexible encoder\n",
    "        encoder_layers = [nn.Linear(self.observation_features, self.encoder_layer_sizes[0]), nn.BatchNorm1d(self.encoder_layer_sizes[0])] # input layer\n",
    "        encoder_layers.append(nn.LeakyReLU()) # Make it non-linear\n",
    "        for i in range(self.n_encoder_layers-1):\n",
    "            encoder_layers.append(nn.Linear(self.encoder_layer_sizes[i], self.encoder_layer_sizes[i+1]))\n",
    "            encoder_layers.append(nn.BatchNorm1d(self.encoder_layer_sizes[i+1]))\n",
    "            encoder_layers.append(nn.LeakyReLU())\n",
    "        encoder_layers.append(nn.Linear(self.encoder_layer_sizes[-1], self.latent_features*2)) # output layer\n",
    "        \n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Defining flexible decoder\n",
    "        decoder_layers = [nn.Linear(self.latent_features, self.decoder_layer_sizes[-1]), nn.BatchNorm1d(self.decoder_layer_sizes[-1])] # input layer\n",
    "        decoder_layers.append(nn.LeakyReLU())\n",
    "        for i in range(self.n_decoder_layers-1, 0, -1):\n",
    "            decoder_layers.append(nn.Linear(self.decoder_layer_sizes[i], self.decoder_layer_sizes[i-1]))\n",
    "            decoder_layers.append(nn.BatchNorm1d(self.decoder_layer_sizes[i-1]))\n",
    "            decoder_layers.append(nn.LeakyReLU())\n",
    "        decoder_layers.append(nn.Linear(self.decoder_layer_sizes[0], self.observation_features*3)) # output layer\n",
    "        \n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        # ------------------------------------------------------------------------------\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features]))) # defines a model state variable, which is not learnable\n",
    "        \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Distributions\n",
    "    # ------------------------------------------------------------------------------\n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(z|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        h_x = self.encoder(x) # compute the parameters of the posterior\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1) # splits h_x (which contains both both mean and std values) into 2 chunks along dim=-1\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:]) # The * unpacks the tuple that -shape returns so that we return each element in the tuple instead (opposite of zip operation)\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_params = self.decoder(z)\n",
    "        px_mu, log_px_sigma, px_logit = px_params.chunk(3, dim=-1)\n",
    "        \n",
    "        return HurdleNormal(px_mu, log_px_sigma, px_logit)\n",
    "        \n",
    "    # ------------------------------------------------------------------------------\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99816522-858e-4e78-a981-f53d3935dd71",
   "metadata": {},
   "source": [
    "## Variational inference\n",
    "##### Code unchanged from course exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622f1ca4-f21b-4e1c-89fc-e8a6956b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "\n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x)) # Probability of seeing input data \"x\" under hurdle normal distribution \"px\"\n",
    "        log_pz = reduce(pz.log_prob(z)) # Prior distribution\n",
    "        log_qz = reduce(qz.log_prob(z)) # Posterior distribution\n",
    "\n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "\n",
    "        elbo = torch.mean(log_px) - kl\n",
    "        beta_elbo = torch.mean(log_px) - self.beta * kl\n",
    "\n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e12729-3d77-49c8-a0aa-3f2b51c1f2bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train and test as functions\n",
    "##### This is in order to be able to use Ray Tune to search for optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb5d826-e28a-4e2f-80b6-fa13417e5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "# Load train and test sets outside of training function as we want to compare hyperparameters between identical sets of train and test\n",
    "train_loader, test_loader = data_loaders(batch_size=256, \n",
    "                                         num_workers=4, \n",
    "                                         pin_memory=True)\n",
    "# Only used to get shape of data\n",
    "data_train = next(iter(train_loader))\n",
    "\n",
    "\n",
    "def train_and_test(config):\n",
    "    \n",
    "    \n",
    "    # Function checkpointing (Ray Tune)\n",
    "    step = 0\n",
    "    loaded_checkpoint = session.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        last_step = loaded_checkpoint.to_dict()[\"step\"]\n",
    "        step = last_step + 1\n",
    "    \n",
    "    # Initialize epochs\n",
    "    num_epochs = 100\n",
    "    epoch = 0\n",
    "\n",
    "    \n",
    "    # Initialize model\n",
    "    vae = VariationalAutoencoder(data_train[0].shape,\n",
    "                                 config['latent_features'], \n",
    "                                 config['encoder_layer_sizes'], \n",
    "                                 config['decoder_layer_sizes'])\n",
    "    \n",
    "    # Check if GPU is available, else use CPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Transfer model to device\n",
    "    vae = vae.to(device)\n",
    "    \n",
    "    # Evaluator: Variational Inference\n",
    "    beta = 1\n",
    "    vi = VariationalInference(beta=beta)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(vae.parameters(), \n",
    "                                 lr = config['lr'])\n",
    "    \n",
    "    # define dictionary to store the training curves\n",
    "    training_data = defaultdict(list)\n",
    "    validation_data = defaultdict(list)\n",
    "    \n",
    "    while epoch < num_epochs:\n",
    "        epoch+= 1\n",
    "        # print(f\"######## Epoch: {epoch} of {num_epochs} ########\")\n",
    "        training_epoch_data = defaultdict(list)\n",
    "        vae.train()\n",
    "\n",
    "        # Go through each batch in the training dataset using the loader\n",
    "        for x in train_loader:\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            # perform a forward pass through the model and compute the ELBO\n",
    "            loss, diagnostics, outputs = vi(vae, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # gather data for the current bach\n",
    "            for k, v in diagnostics.items():\n",
    "                training_epoch_data[k] += [v.mean().item()]\n",
    "\n",
    "\n",
    "        # gather data for the full epoch\n",
    "        for k, v in training_epoch_data.items():\n",
    "            training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "        # Evaluate on a single batch, do not propagate gradients\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "\n",
    "            # Just load a single batch from the test loader\n",
    "            x = next(iter(test_loader))\n",
    "            x = x.to(device)\n",
    "\n",
    "\n",
    "            # perform a forward pass through the model and compute the ELBO\n",
    "            loss, diagnostics, outputs = vi(vae, x)\n",
    "     \n",
    "            \n",
    "            # gather data for the validation step\n",
    "            for k, v in diagnostics.items():\n",
    "                validation_data[k] += [v.mean().item()]\n",
    "\n",
    "\n",
    "        checkpoint = Checkpoint.from_dict({\"step\": step})\n",
    "        session.report({\"test_loss\": validation_data['elbo'][-1],\"train_loss\": training_data['elbo'][-1]}, checkpoint=checkpoint)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930bb22-92cf-42dd-a766-1414e8ea737a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run the grid search using ASHA scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b375831b-0495-4b87-af0c-f627faa166c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 14:06:29,718\tWARNING function_trainable.py:586 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-12-21 14:08:20</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:50.59        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.9/50.1 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=13<br>Bracket: Iter 60.000: -1888.0979410807292 | Iter 20.000: -2512.95751953125<br>Resources requested: 0/16 CPUs, 1.0/1 GPUs, 0.0/29.68 GiB heap, 0.0/14.84 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc              </th><th>decoder_layer_sizes  </th><th>encoder_layer_sizes  </th><th style=\"text-align: right;\">  latent_features</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  test_loss</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_and_test_48c56_00013</td><td>RUNNING   </td><td>172.27.30.56:2474</td><td>[4096, 16384, 4096]  </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1.84945</td><td style=\"text-align: right;\">   -5463.73</td><td style=\"text-align: right;\">    -2580.17</td></tr>\n",
       "<tr><td>train_and_test_48c56_00014</td><td>PENDING   </td><td>                 </td><td>[4096, 4096, 40_aec0 </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00015</td><td>PENDING   </td><td>                 </td><td>[4096, 8192, 81_c1c0 </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00016</td><td>PENDING   </td><td>                 </td><td>[256]                </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00017</td><td>PENDING   </td><td>                 </td><td>[512]                </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00018</td><td>PENDING   </td><td>                 </td><td>[1024]               </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00019</td><td>PENDING   </td><td>                 </td><td>[2048]               </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00020</td><td>PENDING   </td><td>                 </td><td>[4096]               </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00021</td><td>PENDING   </td><td>                 </td><td>[8192]               </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00022</td><td>PENDING   </td><td>                 </td><td>[512, 512]           </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00023</td><td>PENDING   </td><td>                 </td><td>[1024, 1024]         </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00024</td><td>PENDING   </td><td>                 </td><td>[2048, 2048]         </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00025</td><td>PENDING   </td><td>                 </td><td>[4096, 4096]         </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00026</td><td>PENDING   </td><td>                 </td><td>[8192, 8192]         </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00027</td><td>PENDING   </td><td>                 </td><td>[4096, 4096, 4096]   </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00028</td><td>PENDING   </td><td>                 </td><td>[4096, 8192, 4096]   </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00029</td><td>PENDING   </td><td>                 </td><td>[4096, 16384, 4096]  </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00030</td><td>PENDING   </td><td>                 </td><td>[4096, 4096, 40_d1c0 </td><td>[512]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>train_and_test_48c56_00000</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[256]                </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        12.6599 </td><td style=\"text-align: right;\">   -3934.77</td><td style=\"text-align: right;\">    -3938.52</td></tr>\n",
       "<tr><td>train_and_test_48c56_00001</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[512]                </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        10.5467 </td><td style=\"text-align: right;\">   -2691.8 </td><td style=\"text-align: right;\">    -2621.32</td></tr>\n",
       "<tr><td>train_and_test_48c56_00002</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[1024]               </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        10.6576 </td><td style=\"text-align: right;\">   -2224.8 </td><td style=\"text-align: right;\">    -2126.46</td></tr>\n",
       "<tr><td>train_and_test_48c56_00003</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[2048]               </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        10.9911 </td><td style=\"text-align: right;\">   -1907   </td><td style=\"text-align: right;\">    -1897.19</td></tr>\n",
       "<tr><td>train_and_test_48c56_00004</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[4096]               </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        11.7266 </td><td style=\"text-align: right;\">   -1637.8 </td><td style=\"text-align: right;\">    -1568.45</td></tr>\n",
       "<tr><td>train_and_test_48c56_00005</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[8192]               </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        11.3516 </td><td style=\"text-align: right;\">   -1347.55</td><td style=\"text-align: right;\">    -1232.82</td></tr>\n",
       "<tr><td>train_and_test_48c56_00006</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[512, 512]           </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2.05978</td><td style=\"text-align: right;\">   -7425.43</td><td style=\"text-align: right;\">    -6930.75</td></tr>\n",
       "<tr><td>train_and_test_48c56_00007</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[1024, 1024]         </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2.04304</td><td style=\"text-align: right;\">   -5077.32</td><td style=\"text-align: right;\">    -4227.73</td></tr>\n",
       "<tr><td>train_and_test_48c56_00008</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[2048, 2048]         </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2.8136 </td><td style=\"text-align: right;\">   -3260.7 </td><td style=\"text-align: right;\">    -2447.28</td></tr>\n",
       "<tr><td>train_and_test_48c56_00009</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[4096, 4096]         </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        12.0654 </td><td style=\"text-align: right;\">   -1573.34</td><td style=\"text-align: right;\">    -1537.2 </td></tr>\n",
       "<tr><td>train_and_test_48c56_00010</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[8192, 8192]         </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        14.3452 </td><td style=\"text-align: right;\">   -1279.21</td><td style=\"text-align: right;\">    -1245.64</td></tr>\n",
       "<tr><td>train_and_test_48c56_00011</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[4096, 4096, 4096]   </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2.55655</td><td style=\"text-align: right;\">   -2656.03</td><td style=\"text-align: right;\">    -2063.13</td></tr>\n",
       "<tr><td>train_and_test_48c56_00012</td><td>TERMINATED</td><td>172.27.30.56:2474</td><td>[4096, 8192, 4096]   </td><td>[256]                </td><td style=\"text-align: right;\">               32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2.81376</td><td style=\"text-align: right;\">   -2512.96</td><td style=\"text-align: right;\">    -2067.5 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  test_loss</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_and_test_48c56_00000</td><td>2022-12-21_14-06-44</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -3934.77</td><td style=\"text-align: right;\">            12.6599 </td><td style=\"text-align: right;\">         0.111371 </td><td style=\"text-align: right;\">      12.6599 </td><td style=\"text-align: right;\"> 1671628004</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -3938.52</td><td style=\"text-align: right;\">                 100</td><td>48c56_00000</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00001</td><td>2022-12-21_14-06-54</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -2691.8 </td><td style=\"text-align: right;\">            10.5467 </td><td style=\"text-align: right;\">         0.104403 </td><td style=\"text-align: right;\">      10.5467 </td><td style=\"text-align: right;\"> 1671628014</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -2621.32</td><td style=\"text-align: right;\">                 100</td><td>48c56_00001</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00002</td><td>2022-12-21_14-07-05</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -2224.8 </td><td style=\"text-align: right;\">            10.6576 </td><td style=\"text-align: right;\">         0.109168 </td><td style=\"text-align: right;\">      10.6576 </td><td style=\"text-align: right;\"> 1671628025</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -2126.46</td><td style=\"text-align: right;\">                 100</td><td>48c56_00002</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00003</td><td>2022-12-21_14-07-16</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -1907   </td><td style=\"text-align: right;\">            10.9911 </td><td style=\"text-align: right;\">         0.0966794</td><td style=\"text-align: right;\">      10.9911 </td><td style=\"text-align: right;\"> 1671628036</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -1897.19</td><td style=\"text-align: right;\">                 100</td><td>48c56_00003</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00004</td><td>2022-12-21_14-07-28</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -1637.8 </td><td style=\"text-align: right;\">            11.7266 </td><td style=\"text-align: right;\">         0.0959854</td><td style=\"text-align: right;\">      11.7266 </td><td style=\"text-align: right;\"> 1671628048</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -1568.45</td><td style=\"text-align: right;\">                 100</td><td>48c56_00004</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00005</td><td>2022-12-21_14-07-39</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -1347.55</td><td style=\"text-align: right;\">            11.3516 </td><td style=\"text-align: right;\">         0.104327 </td><td style=\"text-align: right;\">      11.3516 </td><td style=\"text-align: right;\"> 1671628059</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -1232.82</td><td style=\"text-align: right;\">                 100</td><td>48c56_00005</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00006</td><td>2022-12-21_14-07-41</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                        20</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -7425.43</td><td style=\"text-align: right;\">             2.05978</td><td style=\"text-align: right;\">         0.100195 </td><td style=\"text-align: right;\">       2.05978</td><td style=\"text-align: right;\"> 1671628061</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -6930.75</td><td style=\"text-align: right;\">                  20</td><td>48c56_00006</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00007</td><td>2022-12-21_14-07-43</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                        20</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -5077.32</td><td style=\"text-align: right;\">             2.04304</td><td style=\"text-align: right;\">         0.10283  </td><td style=\"text-align: right;\">       2.04304</td><td style=\"text-align: right;\"> 1671628063</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -4227.73</td><td style=\"text-align: right;\">                  20</td><td>48c56_00007</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00008</td><td>2022-12-21_14-07-46</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                        20</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -3260.7 </td><td style=\"text-align: right;\">             2.8136 </td><td style=\"text-align: right;\">         0.158887 </td><td style=\"text-align: right;\">       2.8136 </td><td style=\"text-align: right;\"> 1671628066</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -2447.28</td><td style=\"text-align: right;\">                  20</td><td>48c56_00008</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00009</td><td>2022-12-21_14-07-58</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -1573.34</td><td style=\"text-align: right;\">            12.0654 </td><td style=\"text-align: right;\">         0.118652 </td><td style=\"text-align: right;\">      12.0654 </td><td style=\"text-align: right;\"> 1671628078</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -1537.2 </td><td style=\"text-align: right;\">                 100</td><td>48c56_00009</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00010</td><td>2022-12-21_14-08-13</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                       100</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -1279.21</td><td style=\"text-align: right;\">            14.3452 </td><td style=\"text-align: right;\">         0.126583 </td><td style=\"text-align: right;\">      14.3452 </td><td style=\"text-align: right;\"> 1671628093</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -1245.64</td><td style=\"text-align: right;\">                 100</td><td>48c56_00010</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00011</td><td>2022-12-21_14-08-15</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                        20</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -2656.03</td><td style=\"text-align: right;\">             2.55655</td><td style=\"text-align: right;\">         0.113816 </td><td style=\"text-align: right;\">       2.55655</td><td style=\"text-align: right;\"> 1671628095</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -2063.13</td><td style=\"text-align: right;\">                  20</td><td>48c56_00011</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00012</td><td>2022-12-21_14-08-18</td><td>True  </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                        20</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -2512.96</td><td style=\"text-align: right;\">             2.81376</td><td style=\"text-align: right;\">         0.111749 </td><td style=\"text-align: right;\">       2.81376</td><td style=\"text-align: right;\"> 1671628098</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    -2067.5 </td><td style=\"text-align: right;\">                  20</td><td>48c56_00012</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "<tr><td>train_and_test_48c56_00013</td><td>2022-12-21_14-08-19</td><td>False </td><td>                </td><td>bbbeb8d506ff40419795dad4a4d4312e</td><td>DESKTOP-3I4L4RQ</td><td style=\"text-align: right;\">                         1</td><td>172.27.30.56</td><td style=\"text-align: right;\"> 2474</td><td>True               </td><td style=\"text-align: right;\">   -9873.58</td><td style=\"text-align: right;\">             0.76462</td><td style=\"text-align: right;\">         0.76462  </td><td style=\"text-align: right;\">       0.76462</td><td style=\"text-align: right;\"> 1671628099</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">   -13940.4 </td><td style=\"text-align: right;\">                   1</td><td>48c56_00013</td><td style=\"text-align: right;\">   0.00200653</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 14:08:20,339\tWARNING tune.py:705 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2022-12-21 14:08:20,593\tERROR tune.py:773 -- Trials did not complete: [train_and_test_48c56_00013, train_and_test_48c56_00014, train_and_test_48c56_00015, train_and_test_48c56_00016, train_and_test_48c56_00017, train_and_test_48c56_00018, train_and_test_48c56_00019, train_and_test_48c56_00020, train_and_test_48c56_00021, train_and_test_48c56_00022, train_and_test_48c56_00023, train_and_test_48c56_00024, train_and_test_48c56_00025, train_and_test_48c56_00026, train_and_test_48c56_00027, train_and_test_48c56_00028, train_and_test_48c56_00029, train_and_test_48c56_00030]\n",
      "2022-12-21 14:08:20,594\tINFO tune.py:777 -- Total run time: 110.88 seconds (110.55 seconds for the tuning loop).\n",
      "2022-12-21 14:08:20,594\tWARNING tune.py:783 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='test_loss',\n",
    "    mode='max',\n",
    "    max_t=100,\n",
    "    grace_period=20,\n",
    "    reduction_factor=3,\n",
    "    brackets=1)\n",
    "\n",
    "trainable_with_gpu = tune.with_resources(train_and_test, {\"gpu\": 1})\n",
    "\n",
    "# Here we define the hyperparameter search space for our grid search - this is nowhere near exhaustive as we are resource limited.\n",
    "search_space = {    'latent_features': tune.grid_search([32,64,128]), \n",
    "                    'encoder_layer_sizes': tune.grid_search([[256], [512], [1024], [2048], [4096], [8192],\n",
    "                                                            [512,512], [1024,1024], [2048,2048], [4096,4096], [8192,8192],\n",
    "                                                            [4096,4096,4096], [4096,8192,4096], [4096, 16384, 4096],\n",
    "                                                            [4096,4096,4096,4096], [4096,8192,8192,4096]]), \n",
    "                    'decoder_layer_sizes': tune.grid_search([[256], [512], [1024], [2048], [4096], [8192],\n",
    "                                                            [512,512], [1024,1024], [2048,2048], [4096,4096], [8192,8192],\n",
    "                                                            [4096,4096,4096], [4096,8192,4096], [4096, 16384, 4096],\n",
    "                                                            [4096,4096,4096,4096], [4096,8192,8192,4096]]), \n",
    "                    'lr': tune.grid_search([1e-4, 1e-5])\n",
    "               }\n",
    "\n",
    "tuner = tune.Tuner(trainable_with_gpu, \n",
    "                   tune_config=tune.TuneConfig(scheduler=asha_scheduler),\n",
    "                   #metric=\"test_loss\", mode=\"max\", \n",
    "                   run_config=air.RunConfig(name = \"20_subset-04-12-2022\", local_dir=\"./ray_results\"),\n",
    "                   # https://docs.ray.io/en/latest/ray-air/package-ref.html#module-ray.tune.tune_config\n",
    "                   param_space = search_space)\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f02696-0c54-40af-b67c-31e464634f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config is: {'latent_features': 32, 'encoder_layer_sizes': [256], 'decoder_layer_sizes': [8192, 8192], 'lr': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best config is:\", results.get_best_result(metric=\"test_loss\", mode=\"max\").config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045325e-12e7-4f1c-bcb2-97c258ddaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE RESULTS DATAFRAME TO DISK\n",
    "tune_df = results.get_dataframe()\n",
    "tune_df.to_pickle(os.getcwd() + \"/ray_results/20_subset-04-12-2022.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3c085-94e3-47df-8611-64ae16e6e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the full dataframe of tested hyperparameter combinations sorted by lowest test loss\n",
    "tune_df.sort_values(by = ['test_loss'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744dc09-a811-4628-ac3e-1c04e8fd5398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL_Project]",
   "language": "python",
   "name": "conda-env-DL_Project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
